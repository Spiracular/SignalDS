{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Part 1](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-2-word-vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from nltk.corpus import stopwords\n",
    "import nltk.data\n",
    "import logging\n",
    "import numpy as np  # Make sure that numpy is imported\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.cluster import KMeans\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "\n",
    "#from KaggleWord2VecUtility import KaggleWord2VecUtility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximally efficient neurons would be active 1/2 the time (but usually there are other considerations as well).\n",
    "\n",
    "Course coding to get finer resolution is a cool concept.\n",
    "\n",
    "$$saving = \\dfrac{fineneurons}{coarseneurons} = r^{k-1}$$\n",
    "\n",
    "Where r = radius(increase?), and k = dimensions\n",
    "\n",
    "Each neuron defines a boundary (the range of vals in which it is activated)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 25000\n",
      "AddTrain 50000\n",
      "Test 25000\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"data/labeledTrainData.tsv\", header=0, delimiter=\"\\t\",\\\n",
    "                   quoting=3)\n",
    "\n",
    "addtrain = pd.read_csv(\"data/unlabeledTrainData.tsv\", header=0, delimiter=\"\\t\",\\\n",
    "                   quoting=3)\n",
    "test = pd.read_csv(\"data/testData.tsv\", header=0, delimiter=\"\\t\",\\\n",
    "                   quoting=3)\n",
    "print(\"Train {}\".format(train[\"review\"].size))\n",
    "print (\"AddTrain {}\".format(addtrain[\"review\"].size))\n",
    "print(\"Test {}\".format(test[\"review\"].size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def review_to_wordslist(review, rm_stopwords = False):\n",
    "    rev_txt = BeautifulSoup(review, \"lxml\").get_text()\n",
    "    rev_txt = re.sub(\"[^a-zA-Z]\", \" \", rev_txt)\n",
    "    words = rev_txt.lower().split()\n",
    "    if rm_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    return(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load(\"tokenizers/punkt/english.pickle\")\n",
    "#To be used to interpret end-sentence punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Input format expected for Word2Vec: list of words in each sentence\n",
    "#(list of lists)\n",
    "\n",
    "def review_to_sentences(review, tokenizer, rm_stopwords = False):\n",
    "    raw_sentences = tokenizer.tokenize(review.strip())\n",
    "    sentences = []\n",
    "    for raw_sentence in raw_sentences:\n",
    "        if len(raw_sentence)>0:\n",
    "            sentences.append(review_to_wordslist(raw_sentence, rm_stopwords))\n",
    "    return(sentences)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Spirapple/anaconda/lib/python3.5/site-packages/bs4/__init__.py:198: UserWarning: \"b'.'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  '\"%s\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.' % markup)\n",
      "/Users/Spirapple/anaconda/lib/python3.5/site-packages/bs4/__init__.py:207: UserWarning: \"b'http://www.happierabroad.com\"'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/Users/Spirapple/anaconda/lib/python3.5/site-packages/bs4/__init__.py:207: UserWarning: \"b'http://www.archive.org/details/LovefromaStranger\"'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/Users/Spirapple/anaconda/lib/python3.5/site-packages/bs4/__init__.py:207: UserWarning: \"b'http://www.loosechangeguide.com/LooseChangeGuide.html\"'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/Users/Spirapple/anaconda/lib/python3.5/site-packages/bs4/__init__.py:207: UserWarning: \"b'http://www.msnbc.msn.com/id/4972055/site/newsweek/\"'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/Users/Spirapple/anaconda/lib/python3.5/site-packages/bs4/__init__.py:198: UserWarning: \"b'..'\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.\n",
      "  '\"%s\" looks like a filename, not markup. You should probably open this file and pass the filehandle into Beautiful Soup.' % markup)\n",
      "/Users/Spirapple/anaconda/lib/python3.5/site-packages/bs4/__init__.py:207: UserWarning: \"b'http://www.youtube.com/watch?v=a0KSqelmgN8\"'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n",
      "/Users/Spirapple/anaconda/lib/python3.5/site-packages/bs4/__init__.py:207: UserWarning: \"b'http://jake-weird.blogspot.com/2007/08/beneath.html\"'\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.\n",
      "  '\"%s\" looks like a URL. Beautiful Soup is not an HTTP client. You should probably use an HTTP client to get the document behind the URL, and feed that document to Beautiful Soup.' % markup)\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "for review in train[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)\n",
    "    #Note: In this unusual context, append will not behave while += would.\n",
    "for review in addtrain[\"review\"]:\n",
    "    sentences += review_to_sentences(review, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "795538\n"
     ]
    }
   ],
   "source": [
    "print (len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters\n",
    "\n",
    "**Architecture:** skip-gram(default), continuous bag of words (In this context, former is slightly slower but produces better results.)\n",
    "\n",
    "**Training Algorithm:** Heirarchical softmax (default), negative sampling\n",
    "\n",
    "**Downsampling of Frequent Words:** Google suggests .00001 and .001\n",
    "\n",
    "**Word Vector Dimensionality:** More to longer run but better results, 10-100s.\n",
    "\n",
    "**Context/Window Size:** How many words of context taken into consideration\n",
    "\n",
    "**Worker Threads:** Number of parallel processes to run (computer specific, 4-6 works on most computers)\n",
    "\n",
    "**Minimum Word Count:** Limits size of vocabulary to meaningful words; ignores words rarer than number given. 10-100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s',\\\n",
    "    level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "model = word2vec.Word2Vec(sentences, workers=4, size=300, min_count = 40, \\\n",
    "                         window = 10, sample = (1e-3))\n",
    "\n",
    "model.init_sims(replace=True)\n",
    "#This will improve memory efficiency if you aren't going to add more to the model.\n",
    "\n",
    "model_name = \"300features_40minwords_10context\"\n",
    "model.save(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'kitchen'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.doesnt_match(\"man woman child kitchen\".split())\n",
    "#Does \"Which of these things is not like the others?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('amazing', 0.7490564584732056),\n",
       " ('incredible', 0.7045366168022156),\n",
       " ('fantastic', 0.6868585348129272),\n",
       " ('excellent', 0.6560066938400269),\n",
       " ('exceptional', 0.6110841035842896),\n",
       " ('outstanding', 0.6005874276161194),\n",
       " ('great', 0.5825968384742737),\n",
       " ('cool', 0.5789231657981873),\n",
       " ('terrific', 0.5760659575462341),\n",
       " ('fabulous', 0.5533530712127686)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"awesome\")\n",
    "#A quick reminder on source of data: 2x50k IMDB movie reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('feet', 0.7450201511383057),\n",
       " ('leg', 0.6914915442466736),\n",
       " ('arm', 0.6386045217514038),\n",
       " ('fingers', 0.6355388164520264),\n",
       " ('legs', 0.6199488043785095),\n",
       " ('shoulder', 0.6194525361061096),\n",
       " ('neck', 0.616032600402832),\n",
       " ('rope', 0.6142753958702087),\n",
       " ('ankle', 0.6115679740905762),\n",
       " ('chest', 0.6012041568756104)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"foot\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('throne', 4.117465972900391),\n",
       " ('kingdom', 3.589508056640625),\n",
       " ('armies', 3.5479326248168945),\n",
       " ('monarch', 3.4877002239227295),\n",
       " ('hunchback', 3.4299702644348145),\n",
       " ('abbey', 3.4065046310424805),\n",
       " ('coronation', 3.273898124694824),\n",
       " ('conquest', 3.252065658569336),\n",
       " ('emperor', 3.246689558029175),\n",
       " ('zenda', 3.2345950603485107)]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar_cosmul(positive=[\"king\",\"queen\"], negative=[\"good\",\"nice\"], topn=10)\n",
    "#Prisoner of Zenda is a novel where a king gets drugged on his coronation day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('stars', 0.6081321239471436),\n",
       " ('clone', 0.49090951681137085),\n",
       " ('superstar', 0.4442404508590698),\n",
       " ('starring', 0.43132728338241577),\n",
       " ('stardom', 0.416018545627594),\n",
       " ('bride', 0.4143727421760559),\n",
       " ('starred', 0.4119265377521515),\n",
       " ('singer', 0.3935401141643524),\n",
       " ('fame', 0.38502663373947144),\n",
       " ('tyrone', 0.3843412399291992)]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"star\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('coscarelli', 1.8433613777160645),\n",
       " ('cheadle', 1.6986799240112305),\n",
       " ('fitting', 1.6894136667251587),\n",
       " ('bluth', 1.672176480293274),\n",
       " ('frightening', 1.6596710681915283),\n",
       " ('surprising', 1.6522626876831055),\n",
       " ('neglected', 1.6461251974105835),\n",
       " ('remains', 1.6389657258987427),\n",
       " ('sure', 1.6355094909667969),\n",
       " ('tragic', 1.614006519317627)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar_cosmul(\"star\", \"fame\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Part 3](https://www.kaggle.com/c/word2vec-nlp-tutorial/details/part-3-more-fun-with-word-vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(16490, 300)\n"
     ]
    }
   ],
   "source": [
    "#model = Word2Vec.load(\"300features_40minwords_10context\")\n",
    "\n",
    "print (type(model.syn0))\n",
    "print (model.syn0.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model[\"flower\"].shape #300 features for each word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def makeFeatureVec(words, model, num_features):\n",
    "    #Averages all word vectors in a paragraph\n",
    "    nwords=0.\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    #index2word contains the names of words in model vocab\n",
    "    index2word_set = set(model.index2word)\n",
    "    for word in words:\n",
    "        if word in index2word_set:\n",
    "            nwords=nwords+1\n",
    "            featureVec = np.add(featureVec, model[word])\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return(featureVec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    # Given a set of reviews (each one a list of words), calculate \n",
    "    # the average feature vector for each one and return a 2D numpy array \n",
    "    counter = 0.\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "       if counter%1000. == 0.:\n",
    "           print (\"Review {} of {}\".format(counter, len(reviews)))\n",
    "       reviewFeatureVecs[counter] = makeFeatureVec(review, model, \\\n",
    "           num_features)\n",
    "       counter = counter + 1.\n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0.0 of 25000\n",
      "Review 1000.0 of 25000\n",
      "Review 2000.0 of 25000\n",
      "Review 3000.0 of 25000\n",
      "Review 4000.0 of 25000\n",
      "Review 5000.0 of 25000\n",
      "Review 6000.0 of 25000\n",
      "Review 7000.0 of 25000\n",
      "Review 8000.0 of 25000\n",
      "Review 9000.0 of 25000\n",
      "Review 10000.0 of 25000\n",
      "Review 11000.0 of 25000\n",
      "Review 12000.0 of 25000\n",
      "Review 13000.0 of 25000\n",
      "Review 14000.0 of 25000\n",
      "Review 15000.0 of 25000\n",
      "Review 16000.0 of 25000\n",
      "Review 17000.0 of 25000\n",
      "Review 18000.0 of 25000\n",
      "Review 19000.0 of 25000\n",
      "Review 20000.0 of 25000\n",
      "Review 21000.0 of 25000\n",
      "Review 22000.0 of 25000\n",
      "Review 23000.0 of 25000\n",
      "Review 24000.0 of 25000\n",
      "Review 0.0 of 25000\n",
      "Review 1000.0 of 25000\n",
      "Review 2000.0 of 25000\n",
      "Review 3000.0 of 25000\n",
      "Review 4000.0 of 25000\n",
      "Review 5000.0 of 25000\n",
      "Review 6000.0 of 25000\n",
      "Review 7000.0 of 25000\n",
      "Review 8000.0 of 25000\n",
      "Review 9000.0 of 25000\n",
      "Review 10000.0 of 25000\n",
      "Review 11000.0 of 25000\n",
      "Review 12000.0 of 25000\n",
      "Review 13000.0 of 25000\n",
      "Review 14000.0 of 25000\n",
      "Review 15000.0 of 25000\n",
      "Review 16000.0 of 25000\n",
      "Review 17000.0 of 25000\n",
      "Review 18000.0 of 25000\n",
      "Review 19000.0 of 25000\n",
      "Review 20000.0 of 25000\n",
      "Review 21000.0 of 25000\n",
      "Review 22000.0 of 25000\n",
      "Review 23000.0 of 25000\n",
      "Review 24000.0 of 25000\n"
     ]
    }
   ],
   "source": [
    "num_features = 300\n",
    "\n",
    "clean_train_reviews = []\n",
    "\n",
    "for review in train[\"review\"]:\n",
    "    clean_train_reviews.append(review_to_wordslist(review, rm_stopwords=True))\n",
    "\n",
    "trainDataVecs = getAvgFeatureVecs(clean_train_reviews, model, num_features)\n",
    "\n",
    "clean_test_reviews = []\n",
    "for review in test[\"review\"]:\n",
    "    clean_test_reviews.append(review_to_wordslist(review, rm_stopwords=True))\n",
    "\n",
    "testDataVecs = getAvgFeatureVecs(clean_test_reviews, model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Random Forest\n",
    "forest = RandomForestClassifier(n_estimators=100)\n",
    "forest = forest.fit(trainDataVecs, train[\"sentiment\"])\n",
    "result = forest.predict(testDataVecs)\n",
    "\n",
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result})\n",
    "output.to_csv(\"Word2Vec_AverageVectors.csv\", index=False, quoting=3)\n",
    "\n",
    "#tf-idf is a measure of prevalence of word in particular context relative to overall frequency of word,\n",
    "#as an approximation of importance of word to context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken: 1970.5447051525116 seconds\n"
     ]
    }
   ],
   "source": [
    "#K-means clustering\n",
    "\n",
    "##WARNING: Will run for 40mins or more! (this is not a bug)\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "#k = 1/5 vocab size; avg of ~5 words per cluster\n",
    "\n",
    "word_vectors = model.syn0 #syn0 store the feature vectors\n",
    "num_clusters = round(word_vectors.shape[0]/5)\n",
    "\n",
    "kmeans_clustering = KMeans(n_clusters = num_clusters)\n",
    "idx = kmeans_clustering.fit_predict(word_vectors)\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end-start\n",
    "print(\"Time taken: {} seconds\".format(elapsed))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_centroid_map = dict(zip(model.index2word, idx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-106-235fa47b9ab3>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-106-235fa47b9ab3>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    word_centroid_map.1\u001b[0m\n\u001b[0m                      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "word_centroid_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Cluster 0\n",
      "['vegas', 'las']\n",
      "\n",
      " Cluster 1\n",
      "['adorable', 'sassy', 'bitchy', 'feisty', 'perky', 'spunky', 'hush', 'virginal', 'vixen', 'foxy', 'bubbly', 'ditsy', 'tomboy', 'plucky']\n",
      "\n",
      " Cluster 2\n",
      "['foe', 'volunteer', 'ceremonies', 'pakistani']\n",
      "\n",
      " Cluster 3\n",
      "['flair', 'precision', 'panache', 'finesse', 'verve', 'flourishes']\n",
      "\n",
      " Cluster 4\n",
      "['jungle', 'tunnel', 'steam', 'traps', 'corridors', 'sewer', 'tunnels', 'cellar', 'maze', 'portal']\n",
      "\n",
      " Cluster 5\n",
      "['spiritual', 'philosophy', 'morality', 'angst', 'wisdom', 'realities', 'communication', 'deception', 'existential', 'infidelity', 'alienation', 'complexities', 'bourgeois', 'emptiness', 'spirituality', 'idealism', 'uncertainty', 'unspoken', 'conflicting', 'adversity', 'individuality', 'familial']\n",
      "\n",
      " Cluster 6\n",
      "['victor', 'sid', 'homer', 'shepherd', 'reverend', 'jacob', 'bailey', 'emil', 'erik', 'vargas', 'theo', 'sasha', 'calamity', 'fagin', 'blossom', 'trump', 'burrows', 'trent', 'johns', 'celine', 'xavier', 'calvin', 'hickock']\n",
      "\n",
      " Cluster 7\n",
      "['satellite']\n",
      "\n",
      " Cluster 8\n",
      "['change', 'spoil', 'ruin', 'redeem', 'salvage', 'confirm']\n",
      "\n",
      " Cluster 9\n",
      "['natural', 'colorful', 'elegant', 'bold', 'pleasing', 'lush', 'vibrant', 'colourful', 'evocative', 'sparkling', 'robust']\n"
     ]
    }
   ],
   "source": [
    "for cluster in range(0,10):\n",
    "    print (\"\\n Cluster {}\".format(cluster))\n",
    "    words=[]\n",
    "    TFindex = (idx==cluster)\n",
    "    for i in range(0,len(idx)):\n",
    "        if (TFindex[i]):\n",
    "            words.append( model.index2word[i] )\n",
    "    print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_bag_of_centroids(wordlist, word_centroid_map):\n",
    "    #number of clusters = highest cluster index on map\n",
    "    num_centroids = max(word_centroid_map.values() ) + 1\n",
    "    bag_of_centroids = np.zeros(num_centroids, dtype=\"float32\")\n",
    "    \n",
    "    for word in wordlist:\n",
    "        if word in word_centroid_map:\n",
    "            index=  word_centroid_map[word]\n",
    "            bag_of_centroids[index] += 1\n",
    "    return (bag_of_centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_centroids = np.zeros((train[\"review\"].size, num_clusters), dtype=\"float32\")\n",
    "counter=0\n",
    "for review in clean_train_reviews:\n",
    "    train_centroids[counter] = create_bag_of_centroids(review, word_centroid_map)\n",
    "    counter += 1\n",
    "test_centroids = np.zeros((test[\"review\"].size, num_clusters), dtype=\"float32\")\n",
    "\n",
    "counter=0\n",
    "for review in clean_test_reviews:\n",
    "    test_centroids[counter]=create_bag_of_centroids(review, word_centroid_map)\n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(n_estimators=100)\n",
    "forest = forest.fit(train_centroids, train[\"sentiment\"])\n",
    "result = forest.predict(test_centroids)\n",
    "output = pd.DataFrame(data={\"id\":test[\"id\"], \"sentiment\":result})\n",
    "output.to_csv(\"BagOfCentroids.csv\", index=False, quoting=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from gensim.models import Phrases\n",
    "\n",
    "#Results in phrases and bigrams"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
