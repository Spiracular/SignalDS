{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Day 1\n",
    "\n",
    "# [Signal Curriculum](https://github.com/signaldatascience/R-curriculum/blob/master/schedule.md)\n",
    "\n",
    "Working directory work: getwd() and setwd()\n",
    "\n",
    "Note: x in c(1:10) yields 1, ..., 10. Also, indexing of vector begins with 1 in R.\n",
    "\n",
    "[Time Functions in R (StackOverflow)](http://stackoverflow.com/questions/6262203/measuring-function-execution-time-in-r/33375008#33375008)\n",
    "\n",
    "system.time(replicate(1000, function_tracking(arguments)))\n",
    "\n",
    "[Defining an R function](https://www.r-bloggers.com/how-to-write-and-debug-an-r-function/)\n",
    "\n",
    "name.of.function <- function(argument1, argument2) {\n",
    "    statements\n",
    "    return(something)\n",
    "}\n",
    "\n",
    "**Thing I keep screwing up:** numeric(value) means make a BLANK numeric. as.numeric(value) creates numeric from value.\n",
    "\n",
    "## [Memoization](https://en.wikipedia.org/wiki/Memoization)\n",
    "\n",
    "Storing the results of expensive functions \n",
    "\n",
    "### Neat way to do Fibanacci Numbers\n",
    "\n",
    "$$\\begin{bmatrix} 1&1\\\\1&0 \\end{bmatrix} \\begin{bmatrix} F_{n+1}\\\\F_n\\end{bmatrix} = \\begin{bmatrix} F_{n+2}\\\\F_{n+1}\\end{bmatrix}$$\n",
    "\n",
    "$$\\begin{bmatrix} 1&1\\\\1&0 \\end{bmatrix}^n \\begin{bmatrix} 1\\\\1\\end{bmatrix} = \\begin{bmatrix} F_{n+2}\\\\F_{n+1}\\end{bmatrix}$$\n",
    "\n",
    "This runs in $F_n -> O(n)$\n",
    "\n",
    "You can subsequently improve time by using only squares and squares-of-squares to calculate high numbers, getting it down to $F_n -> O(log(n))$.\n",
    "\n",
    "P.S. Don't ask RC R questions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Day 2\n",
    "\n",
    "Oh weird! R automatically parses columns that draw from a small set of strings as categorical variables. (can turn off this behavior with stringsAsFactors = FALSE, but it is performed by default in R)\n",
    "\n",
    "To see if something is a data.frame, use class(df) (its underlying type is list).\n",
    "\n",
    "Combine dataframes using cbind() columnwise and rbind() rowwise.\n",
    "\n",
    "To combine dataframes with different collumn numbers, use plyr::rbind.fill()\n",
    "\n",
    "Please limit yourself to combining data.frame type objects with cbind() or you may see unintended behaviors.\n",
    "\n",
    "Lexical Scoping\n",
    "\n",
    "Dynamic Scoping\n",
    "\n",
    "\n",
    "**x[\"A\"] returns a list with 1 item.**\n",
    "\n",
    "**x\\$A returns the value enclosed in \"A\"**\n",
    "\n",
    "\n",
    "str(A) = structure of list A\n",
    "\n",
    "\\$ is a shorthand for [[ ]] in r\n",
    "\n",
    "# Description of a Normal Distribution\n",
    "\n",
    "$$y = ax + \\epsilon$$\n",
    "$$(\\sigma_\\epsilon = b)$$\n",
    "$$a^2\\sigma_{x}^2 + \\sigma_\\epsilon^2  = \\sigma_y^2$$\n",
    "\n",
    "$$a^2Var(x) + Var(\\epsilon) = Var(y)$$\n",
    "\n",
    "\n",
    "\n",
    "a = slope = correlation (x,y) = $R^2$\n",
    "\n",
    "b = std in error\n",
    "\n",
    "$$$$\n",
    "\n",
    "Assume a purely numeric dataframe that you want to transverse in an angular spiral starting at the upper-left corner and ending up at the center item.\n",
    "\n",
    "Grab entire row or column, sometimes reverse it, remove when grabbed.\n",
    "\n",
    "Avoid index manipulation if you can?\n",
    "\n",
    "R is good for this problem: get leftmost collumn w/ ef[1] and remove column  via ef[-1]; remove row via df[-nrow(df),].\n",
    "\n",
    "Reverse with rev()\n",
    "\n",
    "R has automatic coersion in many circumstances (ex: returning a vector when you slice the last column). Watch out!\n",
    "\n",
    "Avoided by: df[-1,,drop=FALSE]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standard Linear Regression Notation\n",
    "$$y = mx + b + \\epsilon$$\n",
    "\n",
    "$$P(A|B) = \\dfrac{P(B|A)P(A)}{P(B}$$\n",
    " \n",
    "**An intuitive explanation worth noting**\n",
    "\n",
    "$$Var(X) = \\sum_{x \\in X}{((x - \\bar{x})^2)}$$\n",
    "\n",
    "$$\\sigma_x = \\sum_{x \\in X}{|(x - \\bar{x})|} = \\sqrt{Var(x)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# SQL Notes\n",
    "\n",
    "SELECT continent, name, area FROM world x\n",
    "  WHERE area >= ALL\n",
    "    (SELECT area FROM world y\n",
    "        WHERE y.continent=x.continent\n",
    "          AND area>0)\n",
    "          \n",
    "SELECT continent, name FROM world x\n",
    "   WHERE name = \n",
    "      (SELECT MIN(name) FROM world y WHERE x.continent = y.continent)\n",
    "\n",
    "          \n",
    "/* MIN(name) OR SELECT TOP 1 name WHERE ___ ORDER BY name*/\n",
    "\n",
    "scrap, not useful: /*So... for each continent check all name in country for population<=25M */\n",
    "/*NOT, ANY, ALL ? */\n",
    "\n",
    "Thing that worked...\n",
    "SELECT name, continent, population FROM world x\n",
    "   WHERE continent NOT IN\n",
    "      (SELECT continent FROM world y\n",
    "      WHERE x.continent = y.continent\n",
    "      AND population >25000000)\n",
    "\n",
    "\n",
    "SELECT name, continent, population FROM world x\n",
    "   WHERE continent NOT IN\n",
    "      (SELECT continent FROM world y\n",
    "      WHERE x.continent = y.continent\n",
    "      AND population >25000000)\n",
    "      \n",
    "\n",
    "   AND x.name != y.name\n",
    "   \n",
    " Use max in some way?\n",
    " \n",
    "these did not work\n",
    "SELECT TOP 1, TOP 2 FROM (spit out ordered database here for each continent)\n",
    "\n",
    "SELECT name, continent, population FROM world x\n",
    "   WHERE name IN\n",
    "(SELECT name FROM world y\n",
    "   WHERE x.continent = y.continent\n",
    "   AND x.name != y.name\n",
    "   AND x.population > 3*y.population)\n",
    "   \n",
    "this did work\n",
    "SELECT name, continent FROM world x\n",
    "   WHERE population >  \n",
    "      ALL (SELECT 3*population FROM world y \n",
    "      WHERE x.name != y.name \n",
    "      AND x.continent = y.continent)\n",
    "      **Note: they prefer y.name != x.name notation, and I agree it's probably better.**\n",
    "      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectorise Your Code\n",
    "aka Advanced R Day 5\n",
    "\n",
    "Why?\n",
    "- Useful as an abstraction\n",
    "- Loops in vectorised functions are written in C instead of R (much faster)\n",
    "\n",
    "Examples:\n",
    "- rowSums\n",
    "- colSums\n",
    "- rowMeans\n",
    "- colMeans\n",
    "- cumSum\n",
    "- diff\n",
    "- vapply\n",
    "\n",
    "\n",
    "?lookup tables? (seems similar to dictionary, and it's apparently a fast operation)\n",
    "\n",
    "match and iteger subsetting is faster than rownames and character subsetting?\n",
    "\n",
    "extracting or replacing values in scattered locations on matrix or df? subset w/integer matrix.\n",
    "\n",
    "Matrix algebra executed highly efficiently via external library BLAS.\n",
    "\n",
    "Downside of vectorization: harder to predict time when things scale up. Could be in a favorable direction, though.\n",
    "\n",
    "Can write own vectorised function in C++ w/ Rcpp\n",
    "\n",
    "side-note: %in% is a function\n",
    "\n",
    "apply always turns its data into a matrix. This makes inserting dfs into it inadvisable from a speed standpoint.\n",
    "\n",
    "You can do some useful work for your funciton by predefining things like...\n",
    "- colClasses for read.csv()\n",
    "- levels for factor()\n",
    "- findInterval() labels for cut() (or labels=FALSE)\n",
    "- unlist(x, use.names=FALSE) is faster than just unlist(x)\n",
    "- interaction(drop=TRUE) runs faster\n",
    "\n",
    "?method dispatch? (apparently computationally expensive) ?related to dynamism of R as a language?\n",
    "\n",
    "Method dispatch: Determining which variant of a method to call for the input provided \n",
    "\n",
    "side-note: S4 is a newer set of object types in R. Access subsets in S4 using object@\"name\" while S3 uses object$name and relies heavily on lists. Not terrifically likely to come up anytime soon, but interesting side-note to maybe pursue later.\n",
    "\n",
    "Speeding up functions by bypassing method dispatch entirely:\n",
    "- S3: cal generic.class() instead of generic\n",
    "- S4: findMethod() to pick specific dispatch, save it to a variable, and call that variable\n",
    "\n",
    "mean.default(numeric_vector): faster than just mean(), but **less failsafe** if you feed it a non-numeric.\n",
    "\n",
    "Faster way to turn numeric vectors into a dataframe (as.data.frame is safe vs. data types, but slow and relies on rbind())\n",
    "But if you hand it the wrong data type, you get a corrupted dataframe and a series of warnings.\n",
    "\n",
    "quickdf = function(n){\n",
    "  class(n) = \"data.frame\"\n",
    "  #Yes! Apparently you can do this!\n",
    "  attr(n, \"row.names\") = .set_row_names(length(n[[1]]))\n",
    "  1\n",
    "}\n",
    "\n",
    "\n",
    "*Person writes about how he rewrote the source code of as.data.frame.list() to yield a variant optimized for speed. Recommends other people look up source code when they need to take something extant and make something differently-optimized, or to make something that makes different expectations of the data than the norm.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solutions for Self-Assessment 1\n",
    "\n",
    "Sept 19, 2016\n",
    "\n",
    "(Huh, okay, noted: Will finished well before me, Michelle and Micah took another 2 hours to finish.)\n",
    "\n",
    "X = runif(n_trials)\n",
    "Y = runif(n_trials, max=X)\n",
    "\n",
    "qplot(X,Y)\n",
    "\n",
    "NOTE THIS!\n",
    "df = select(df, Extraversion, Neuroticism, active:scornful)\n",
    "\n",
    "lm(Neuroticism ~ . - Extraversion, df)\n",
    "\n",
    "### SQL section\n",
    "\n",
    "SELECT Salary\n",
    "FROM (\n",
    "    SELECT Salary\n",
    "    FROM Employees\n",
    "    ORDER BY Salary DESC\n",
    "    LIMIT 2\n",
    ")\n",
    "ORDER BY \n",
    "\n",
    "\n",
    "CROSS JOIN (maximally inclusive join, potentially lots of NULLS)\n",
    "INNER JOIN require some key = some key, no generated join NULLS\n",
    "\n",
    "LEFT JOIN: anything in left table sticks, may result in NULLS in columns from right table.\n",
    "\n",
    "(Not following, look at the [wikipedia article](https://en.wikipedia.org/wiki/Join_(SQL)))\n",
    "\n",
    "Solutions in same folder as self-assessment.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "(SELECT name, SUM (leadrole),\n",
    "CASE WHEN ord !=1 THEN 1 ELSE 0 END\n",
    "#make sure to fuse on person\n",
    " FROM\n",
    "(SELECT yr,COUNT(title) AS c FROM\n",
    "   movie JOIN casting ON movie.id=movieid JOIN actor ON actorid=actor.id\n",
    " WHERE name='John Travolta'\n",
    " GROUP BY yr) AS t\n",
    ")\n",
    "\n",
    "ORDER BY actor.name\n",
    "#What I want to do is sort by name and then apply count(movieid) \n",
    "#applied over a list of unique \n",
    "\n",
    "SELECT name FROM\n",
    "  movie JOIN casting ON movie.id=movieid INNER JOIN actor ON actorid=actor.id\n",
    "GROUP BY name\n",
    "HAVING ord = 1  & COUNT(movie.id) > 30\n",
    "COUNT("
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Coefficients\n",
    "\n",
    "In a bid to understand Pearson Correlation Coefficients and what distinguishes them from Polychoric Corellation Coefficients \n",
    "\n",
    "(Some words I'm trying to understand: Pearson's assumesa an underlying joint normal distribution, Polychoric can make other assumptions (I'm presuming Binomial, in the instance I'm trying to mess with?))\n",
    "\n",
    "$$PearCorCoef(X,Y) = \\dfrac{cov(X,Y)}{\\sigma_X \\sigma_Y}$$\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d4/Correlation_examples2.svg/400px-Correlation_examples2.svg.png)\n",
    "\n",
    "<center>Covariance Function (aka Kernel?)</center>\n",
    "$$Cov$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quick notes on factor analysis (from Wikipedia)\n",
    "\n",
    "Exploratory Factor Analysis (EFA) can fairly be considered a more complicated variant of PCA, where factor analysis also uses error terms (and would consider the eigenvalues of PCA to be inflated component loadings contaminated with error variance).\n",
    "\n",
    "FA assumes there are a small number of independent underlying (unseen) variables that determine joint responses in sets of the observed variables. This independence assumption makes it unsuitable to biology. It's practically not used in physics, bio, or chem. It does get used in psychometrics personality theories, marketing, product management, and operations research.\n",
    "\n",
    "$$x-\\mu  = LF + \\epsilon$$\n",
    "\n",
    "With $F$ as unobserved random variables, $\\mu_i = mean(x_i)$, L = matrix of constant modifiers $l_i$ for $x_i$, and $\\epsilon_i$ as stochastic error terms for each item.\n",
    "\n",
    "The number of factors is picked beforehand, as $k<max(i)$, and $F_{1 \\ to \\ k}$ aims to explain as much of the *shared* variance as it can in that many linear combinations (unlike PCA, it likely will not capture all of the variance.)\n",
    "\n",
    "EFA is a generative model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Words from R's optim function\n",
    "(parse and look up later)\n",
    "\n",
    "\"General-purpose optimization based on Nelderâ€“Mead, quasi-Newton and conjugate-gradient algorithms. It includes an option for box-constrained optimization and simulated annealing.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "## Clustering covered by assignment\n",
    "- Heirarchical clustering\n",
    "- K-means clustering\n",
    "- Mixture Models\n",
    "    - Univariate Mixture Model\n",
    "    - Parametric\n",
    "    - Semiparametric\n",
    "    - Non-Parametric\n",
    "- Kohonen Net (? neural nets plus k-means? confused)\n",
    "\n",
    "## New Clustering Methods\n",
    "\n",
    "- Genetic algorithms\n",
    "- Simulated annealing,\n",
    "- Tabu search\n",
    "- Randomized branch-and-bound\n",
    "- Hybrid search"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
